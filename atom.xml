<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yqzzxlj</title>
  
  <subtitle>hello world</subtitle>
  <link href="http://yoursite.com/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-10-29T11:15:30.276Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Yqzzxlj</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>buttfly主题</title>
    <link href="http://yoursite.com/2020/10/29/butterfly%E4%B8%BB%E9%A2%98/"/>
    <id>http://yoursite.com/2020/10/29/butterfly%E4%B8%BB%E9%A2%98/</id>
    <published>2020-10-29T08:06:54.000Z</published>
    <updated>2020-10-29T11:15:30.276Z</updated>
    
    <content type="html"><![CDATA[<pre><code>asdf</code></pre><p>$S_a$</p>]]></content>
    
    
      
      
    <summary type="html">&lt;pre&gt;&lt;code&gt;asdf&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;$S_a$&lt;/p&gt;
</summary>
      
    
    
    
    <category term="语法" scheme="http://yoursite.com/categories/%E8%AF%AD%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>栈和队列</title>
    <link href="http://yoursite.com/2020/06/08/%E6%A0%88%E5%92%8C%E9%98%9F%E5%88%97/"/>
    <id>http://yoursite.com/2020/06/08/%E6%A0%88%E5%92%8C%E9%98%9F%E5%88%97/</id>
    <published>2020-06-08T06:43:21.000Z</published>
    <updated>2020-10-29T11:15:30.276Z</updated>
    
    <content type="html"><![CDATA[<h2 id="栈"><a href="#栈" class="headerlink" title="栈"></a>栈</h2><p>后进先出</p><a id="more"></a>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;栈&quot;&gt;&lt;a href=&quot;#栈&quot; class=&quot;headerlink&quot; title=&quot;栈&quot;&gt;&lt;/a&gt;栈&lt;/h2&gt;&lt;p&gt;后进先出&lt;/p&gt;</summary>
    
    
    
    <category term="algorithm" scheme="http://yoursite.com/categories/algorithm/"/>
    
    
  </entry>
  
  <entry>
    <title>链表问题</title>
    <link href="http://yoursite.com/2020/06/07/%E9%93%BE%E8%A1%A8%E9%97%AE%E9%A2%98/"/>
    <id>http://yoursite.com/2020/06/07/%E9%93%BE%E8%A1%A8%E9%97%AE%E9%A2%98/</id>
    <published>2020-06-07T10:58:47.000Z</published>
    <updated>2020-10-29T11:15:30.276Z</updated>
    
    
    
    
    <category term="algorithm" scheme="http://yoursite.com/categories/algorithm/"/>
    
    
  </entry>
  
  <entry>
    <title>最长回文子串</title>
    <link href="http://yoursite.com/2020/03/19/%E6%9C%80%E9%95%BF%E5%9B%9E%E6%96%87%E5%AD%90%E4%B8%B2/"/>
    <id>http://yoursite.com/2020/03/19/%E6%9C%80%E9%95%BF%E5%9B%9E%E6%96%87%E5%AD%90%E4%B8%B2/</id>
    <published>2020-03-19T03:51:48.000Z</published>
    <updated>2020-10-29T11:15:30.276Z</updated>
    
    <content type="html"><![CDATA[<p>给定一个字符串 s，找到 s 中最长的回文子串。你可以假设 s 的最大长度为 1000。</p><h4 id="示例-1"><a href="#示例-1" class="headerlink" title="示例 1"></a>示例 1</h4><ul><li>输入: “babad”</li><li>输出: “bab”</li><li>注意: “aba” 也是一个有效答案。</li></ul><h4 id="示例-2"><a href="#示例-2" class="headerlink" title="示例 2"></a>示例 2</h4><ul><li>输入: “cbbd”</li><li>输出: “bb”</li></ul><p>在leetcode上看到了５种解法，在这里简单写一下。<a href="https://leetcode-cn.com/problems/longest-palindromic-substring/solution/zui-chang-hui-wen-zi-chuan-c-by-gpe3dbjds1/">原答案地址</a></p><a id="more"></a><h3 id="1-暴力法"><a href="#1-暴力法" class="headerlink" title="1. 暴力法"></a>1. 暴力法</h3><p>找到所有可能的子串，判断是不是回文串。</p><h3 id="2-最长公共子串"><a href="#2-最长公共子串" class="headerlink" title="2. 最长公共子串"></a>2. 最长公共子串</h3><p>将原字符串翻转，并寻找最长的公共子串，注意子串位置要对应，或者检查子串是不是回文串。<br>最长公共子串问题。</p><h3 id="3-动态规划"><a href="#3-动态规划" class="headerlink" title="3. 动态规划"></a>3. 动态规划</h3><p>找到状态转移方程,和初始状态</p><pre><code class="c++">class Solution &#123;public:    string longestPalindrome(string s) &#123;        int len=s.size();        if(len==0||len==1)            return s;        int start=0;//回文串起始位置        int max=1;//回文串最大长度        vector&lt;vector&lt;int&gt;&gt;  dp(len,vector&lt;int&gt;(len));//定义二维动态数组        for(int i=0;i&lt;len;i++)//初始化状态        &#123;            dp[i][i]=1;            if(i&lt;len-1&amp;&amp;s[i]==s[i+1])            &#123;                dp[i][i+1]=1;                max=2;                start=i;            &#125;        &#125;        for(int l=3;l&lt;=len;l++)//l表示检索的子串长度，等于3表示先检索长度为3的子串        &#123;            for(int i=0;i+l-1&lt;len;i++)            &#123;                int j=l+i-1;//终止字符位置                if(s[i]==s[j]&amp;&amp;dp[i+1][j-1]==1)//状态转移                &#123;                    dp[i][j]=1;                    start=i;                    max=l;                &#125;            &#125;        &#125;        return s.substr(start,max);//获取最长回文子串    &#125;&#125;;</code></pre><h3 id="4-中心扩展算法"><a href="#4-中心扩展算法" class="headerlink" title="4. 中心扩展算法"></a>4. 中心扩展算法</h3><pre><code class="c++">class Solution &#123;public:    string longestPalindrome(string s) &#123;        int len=s.size();        if(len==0||len==1)            return s;        int start=0;//记录回文子串起始位置        int end=0;//记录回文子串终止位置        int mlen=0;//记录最大回文子串的长度        for(int i=0;i&lt;len;i++)        &#123;            int len1=expendaroundcenter(s,i,i);//一个元素为中心            int len2=expendaroundcenter(s,i,i+1);//两个元素为中心            mlen=max(max(len1,len2),mlen);            if(mlen&gt;end-start+1)            &#123;                start=i-(mlen-1)/2;                end=i+mlen/2;            &#125;        &#125;        return s.substr(start,mlen);        //该函数的意思是获取从start开始长度为mlen长度的字符串    &#125;private:    int expendaroundcenter(string s,int left,int right)    //计算以left和right为中心的回文串长度    &#123;        int L=left;        int R=right;        while(L&gt;=0 &amp;&amp; R&lt;s.length() &amp;&amp; s[R]==s[L])        &#123;            L--;            R++;        &#125;        return R-L-1;    &#125;&#125;;</code></pre><h3 id="5-Manacher算法"><a href="#5-Manacher算法" class="headerlink" title="5. Manacher算法"></a>5. Manacher算法</h3><pre><code class="c++">class Solution &#123;public:    string longestPalindrome(string s) &#123;       if (s.empty()) return &quot;&quot;;        string manStr = maStr(s);        // 存以i点为中心时的回文半径        vector&lt;int&gt; pVec(manStr.size());        // pR的中心        int pC = -1;        // 回文右边界        int pR = -1;        int maxLength = INT_MIN;        int center = -1;        for (int i = 0; i &lt; manStr.size(); ++i) &#123;            // 如果i在pR里，可以让pVec[i]直接等于其关于pC对称的点，如果这个值超过了pR,就取pR - i;            pVec[i] = pR &gt; i ? min(pVec[pC * 2 - i], pR - i) : 1;            while (i + pVec[i] &lt; manStr.size() &amp;&amp; i - pVec[i] &gt; -1) &#123;                if (manStr[i + pVec[i]] == manStr[i - pVec[i]]) &#123;                    ++pVec[i];                &#125; else &#123;                    break;                &#125;            &#125;            if (i + pVec[i] &gt; pR) &#123;               pR = i + pVec[i];               pC = i;            &#125;            if (pVec[i] &gt; maxLength) &#123;                center = pC;                maxLength = pVec[i];            &#125;        &#125;        int start = (center - maxLength + 1) / 2;        return s.substr(start, maxLength - 1);    &#125;    string maStr(const string&amp; s) &#123;        string ret(s.size() * 2 + 1, &#39;#&#39;);        for (int i = 0; i &lt; s.size(); ++i) &#123;            ret[i * 2 + 1] = s[i];        &#125;        return ret;    &#125;&#125;;</code></pre>]]></content>
    
    
    <summary type="html">&lt;p&gt;给定一个字符串 s，找到 s 中最长的回文子串。你可以假设 s 的最大长度为 1000。&lt;/p&gt;
&lt;h4 id=&quot;示例-1&quot;&gt;&lt;a href=&quot;#示例-1&quot; class=&quot;headerlink&quot; title=&quot;示例 1&quot;&gt;&lt;/a&gt;示例 1&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;输入: “babad”&lt;/li&gt;
&lt;li&gt;输出: “bab”&lt;/li&gt;
&lt;li&gt;注意: “aba” 也是一个有效答案。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;示例-2&quot;&gt;&lt;a href=&quot;#示例-2&quot; class=&quot;headerlink&quot; title=&quot;示例 2&quot;&gt;&lt;/a&gt;示例 2&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;输入: “cbbd”&lt;/li&gt;
&lt;li&gt;输出: “bb”&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在leetcode上看到了５种解法，在这里简单写一下。&lt;a href=&quot;https://leetcode-cn.com/problems/longest-palindromic-substring/solution/zui-chang-hui-wen-zi-chuan-c-by-gpe3dbjds1/&quot;&gt;原答案地址&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="刷题笔记" scheme="http://yoursite.com/categories/%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>动态规划思路</title>
    <link href="http://yoursite.com/2020/03/11/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%80%9D%E8%B7%AF/"/>
    <id>http://yoursite.com/2020/03/11/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%80%9D%E8%B7%AF/</id>
    <published>2020-03-11T12:57:58.000Z</published>
    <updated>2020-10-29T11:15:30.276Z</updated>
    
    <content type="html"><![CDATA[<h3 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h3><ul><li><p><strong>动态规划问题的一般形式就是求值，求解动态规划的核心问题是穷举</strong></p></li><li><p><strong>存在重叠子问题（需要“备忘录”或者“DP Table”）</strong></p></li><li><p><strong>具备最优子结构</strong></p></li><li><p><strong>需要列出正确的“状态转移方程”</strong></p><ul><li>确定状态</li><li>确定dp函数的定义<ol><li><strong>遍历的过程中，所需的状态必须是已经计算出来的</strong></li><li><strong>遍历的终点必须是存储结果的那个位置</strong></li></ol></li><li>确定选择并择优</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;动态规划&quot;&gt;&lt;a href=&quot;#动态规划&quot; class=&quot;headerlink&quot; title=&quot;动态规划&quot;&gt;&lt;/a&gt;动态规划&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;动态规划问题的一般形式就是求值，求解动态规划的核心问题是穷举&lt;/strong&gt;&lt;/p&gt;
&lt;</summary>
      
    
    
    
    <category term="algorithm" scheme="http://yoursite.com/categories/algorithm/"/>
    
    
  </entry>
  
  <entry>
    <title>二叉树遍历</title>
    <link href="http://yoursite.com/2020/03/11/%E4%BA%8C%E5%8F%89%E6%A0%91%E9%81%8D%E5%8E%86/"/>
    <id>http://yoursite.com/2020/03/11/%E4%BA%8C%E5%8F%89%E6%A0%91%E9%81%8D%E5%8E%86/</id>
    <published>2020-03-11T03:14:24.000Z</published>
    <updated>2020-10-29T11:15:30.276Z</updated>
    
    <content type="html"><![CDATA[<h2 id="二叉树递归遍历"><a href="#二叉树递归遍历" class="headerlink" title="二叉树递归遍历"></a>二叉树递归遍历</h2><pre><code class="c++">void traverse(TreeNode* root) &#123;  if(root == nullptr) &#123;    return;  &#125;  traverse(root-&gt;left);  traverse(root-&gt;right);&#125;</code></pre><a id="more"></a><h2 id="二叉树非递归遍历"><a href="#二叉树非递归遍历" class="headerlink" title="二叉树非递归遍历"></a>二叉树非递归遍历</h2><h3 id="中序遍历"><a href="#中序遍历" class="headerlink" title="中序遍历"></a>中序遍历</h3><p>用栈，当前结点非空入站,移动到左孩子。为空赋值为栈顶结点，出栈结点的左子树已经访问结束，继续访问右子树，当前结点移动到右子树。</p><pre><code class="c++">void inTraverse(TreeNode* root) &#123;  if(root != nullptr) &#123;    stack&lt;TreeNode*&gt; s;    while(!s.empty() || root != nullptr) &#123;      if(root) &#123;        s.push(root);        root = root-&gt;left;      &#125; else &#123;        root = s.top();        s.pop();        // root-&gt;val;        root = root-&gt;right;      &#125;    &#125;  &#125;&#125;</code></pre><h3 id="前序遍历"><a href="#前序遍历" class="headerlink" title="前序遍历"></a>前序遍历</h3><p>用栈，中左右的顺序。先访问当前结点，右结点压栈，左结点压栈。然后左结点弹出，访问左子树。</p><pre><code class="c++">void preTraverse(TreeNode* root) &#123;  if(root != nullptr) &#123;    stack&lt;TreeNode*&gt; s;    s.push(root);    while(!s.empty()) &#123;      root = s.top();      s.pop();      // root-&gt;val;      if(root-&gt;right != nullptr) &#123;        s.push(root-&gt;right);      &#125;      if(root-&gt;left != nullptr) &#123;        s.push(root-&gt;left);      &#125;    &#125;  &#125;&#125;</code></pre><h3 id="后序遍历"><a href="#后序遍历" class="headerlink" title="后序遍历"></a>后序遍历</h3><p>用栈，左右中的顺序，判断要访问哪一个。</p><pre><code class="c++">void postorderTraversal(TreeNode* root) &#123;  if (root != nullptr) &#123;    stack&lt;TreeNode*&gt; s;    s.push(root);    // h 为最近打印过的元素, 将ｈ初始化为root，避免ｈ初始为c的左右孩子，(为使代码简洁可复用root,但为了更好理解用两个变量)    TreeNode* h = root;    // c 为栈顶元素,待执行元素，判断访问ｃ还是将其子结点压栈    TreeNode* c = nullptr;    while (!s.empty()) &#123;      c = s.top();      // 由于是后序遍历(左右中), c 的左右孩子都不是最近打印的结点，说明左右ｃ的左右子树都没打印，应先打印ｃ的左子树，将左子结点压栈（先打印左子树）      if (c-&gt;left != nullptr &amp;&amp; h != c-&gt;left &amp;&amp; h != c-&gt;right) &#123;        s.push(c-&gt;left);      // 由于没有执行分支１，说明ｃ的左孩子要么不存在，要么打印过左子树，要么打印过右子树，如果没打印过右子树说明是前两种情况，此时应该打印右子树，将右孩子压栈，      &#125; else if (c-&gt;right != nullptr &amp;&amp; h != c-&gt;right) &#123;        s.push(c-&gt;right);      // 左右子树打印完毕，打印ｃ结点，ｃ结点出栈，更新ｈ。      &#125; else &#123;        // c-&gt;val        s.pop();        h = c;      &#125;    &#125;  &#125;&#125;</code></pre><h3 id="层次遍历"><a href="#层次遍历" class="headerlink" title="层次遍历"></a>层次遍历</h3><p>用队列，访问当前结点的时候将子结点加进队尾</p><pre><code class="c++">void levelTraverse(TreeNode* root) &#123;  if(root != nullptr) &#123;    queue&lt;TreeNode*&gt; q;    q.push(root);    while(!q.empty()) &#123;      root = q.front();      q.pop();      // root-&gt;val      if(root-&gt;left != nullptr) &#123;        q.push(root-&gt;left);      &#125;      if(root-&gt;right != nullptr) &#123;        q.push(root-&gt;right);      &#125;    &#125;  &#125;&#125;</code></pre><h3 id="Morris遍历"><a href="#Morris遍历" class="headerlink" title="Morris遍历"></a>Morris遍历</h3><p>不用栈，用有限个变量访问整棵树。将当前结点左子树的最右结点指向当前结点。这样遍历完左子树就<strong>又回到</strong>了当前结点</p><pre><code class="c++">void morrisTraverse(TreeNode* root) &#123;  if(root == nullptr) return;  TreeNode* cur = root;  TreeNode* mostRight = nullptr;  while(cur != nullptr) &#123;    mostRight = cur-&gt;left;    if (mostRight != nullptr) &#123;      while(mostRight-&gt;right != nullptr &amp;&amp; mostRight-&gt;right != cur)&#123;        mostRight = mostRight-&gt;right;      &#125;      if(mostRight-&gt;right = nullptr) &#123;        mostRight-&gt;right = cur;        cur = cur-&gt;left;        continue;      &#125; else &#123;        mostRight-&gt;right = nullptr;      &#125;    &#125;    cur = cur-&gt;right;  &#125;&#125;</code></pre><h4 id="Morris先序遍历"><a href="#Morris先序遍历" class="headerlink" title="Morris先序遍历"></a>Morris先序遍历</h4><p>在cur指针第一次来到该结点的时候处理结点(指向左孩子之前,或者当左孩子为空时)</p><pre><code class="c++">void morrisTraverse(TreeNode* root) &#123;  if(root == nullptr) return;  TreeNode* cur = root;  TreeNode* mostRight = nullptr;  while(cur != nullptr) &#123;    mostRight = cur-&gt;left;    if (mostRight != nullptr) &#123;      while(mostRight-&gt;right != nullptr &amp;&amp; mostRight-&gt;right != cur)&#123;        mostRight = mostRight-&gt;right;      &#125;      if(mostRight-&gt;right = nullptr) &#123;        mostRight-&gt;right = cur;        // cur-&gt;val;        cur = cur-&gt;left;        continue;      &#125; else &#123;        mostRight-&gt;right = nullptr;      &#125;    &#125; else &#123;      // cur-&gt;val;    &#125;    cur = cur-&gt;right;  &#125;&#125;</code></pre><h4 id="Morris中序遍历"><a href="#Morris中序遍历" class="headerlink" title="Morris中序遍历"></a>Morris中序遍历</h4><p>在cur指针第二次来到当前结点时处理该结点（指向右结点之前）</p><pre><code class="c++">void morrisTraverse(TreeNode* root) &#123;  if(root == nullptr) return;  TreeNode* cur = root;  TreeNode* mostRight = nullptr;  while(cur != nullptr) &#123;    mostRight = cur-&gt;left;    if (mostRight != nullptr) &#123;      while(mostRight-&gt;right != nullptr &amp;&amp; mostRight-&gt;right != cur)&#123;        mostRight = mostRight-&gt;right;      &#125;      if(mostRight-&gt;right = nullptr) &#123;        mostRight-&gt;right = cur;        cur = cur-&gt;left;        continue;      &#125; else &#123;        mostRight-&gt;right = nullptr;      &#125;    &#125;    // cur-&gt;val;    cur = cur-&gt;right;  &#125;&#125;</code></pre><h4 id="Morris后序遍历"><a href="#Morris后序遍历" class="headerlink" title="Morris后序遍历"></a>Morris后序遍历</h4><pre><code class="c++">void morrisTraverse(TreeNode* root) &#123;  if(root == nullptr) return;  TreeNode* cur = root;  TreeNode* mostRight = nullptr;  while(cur != nullptr) &#123;    mostRight = cur-&gt;left;    if (mostRight != nullptr) &#123;      while(mostRight-&gt;right != nullptr &amp;&amp; mostRight-&gt;right != cur)&#123;        mostRight = mostRight-&gt;right;      &#125;      if(mostRight-&gt;right = nullptr) &#123;        mostRight-&gt;right = cur;        cur = cur-&gt;left;        continue;      &#125; else &#123;        mostRight-&gt;right = nullptr;        // pushRightEdge(cur-&gt;left, vector&lt;int&gt;&amp;)      &#125;    &#125;    cur = cur-&gt;right;  &#125;  // pushRightEdge(cur-&gt;left, vector&lt;int&gt;&amp;)  // return ret;&#125;vector&lt;int&gt; pushRightEdge(TreeNode* node, vector&lt;int&gt;&amp; ret) &#123;  TreeNode* tail = reverse(node);  TreeNode* cur = tail;  while(cur != nullptr) &#123;    ret.push_back(cur);    cur = cur-&gt;right;  &#125;  reverse(tail);  return ret;&#125;TreeNode* reverse(TreeNode* node) &#123;  TreeNode* pre = nullptr;  TreeNode* next = nullptr;  TreeNode* cur = node;  while(cur != nullptr) &#123;    next = cur-&gt;right;    cur-&gt;right = pre;    pre = cur;    cur = next;  &#125;  return pre;&#125;</code></pre>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;二叉树递归遍历&quot;&gt;&lt;a href=&quot;#二叉树递归遍历&quot; class=&quot;headerlink&quot; title=&quot;二叉树递归遍历&quot;&gt;&lt;/a&gt;二叉树递归遍历&lt;/h2&gt;&lt;pre&gt;&lt;code class=&quot;c++&quot;&gt;void traverse(TreeNode* root) &amp;#123;
  if(root == nullptr) &amp;#123;
    return;
  &amp;#125;
  traverse(root-&amp;gt;left);
  traverse(root-&amp;gt;right);
&amp;#125;&lt;/code&gt;&lt;/pre&gt;</summary>
    
    
    
    <category term="algorithm" scheme="http://yoursite.com/categories/algorithm/"/>
    
    <category term="树" scheme="http://yoursite.com/categories/algorithm/%E6%A0%91/"/>
    
    
  </entry>
  
  <entry>
    <title>自编码、DQN、GAN网络学习</title>
    <link href="http://yoursite.com/2020/03/10/%E8%87%AA%E7%BC%96%E7%A0%81%E3%80%81DQN%E3%80%81GAN%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2020/03/10/%E8%87%AA%E7%BC%96%E7%A0%81%E3%80%81DQN%E3%80%81GAN%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0/</id>
    <published>2020-03-10T07:54:30.000Z</published>
    <updated>2020-10-29T11:15:30.276Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/MorvanZhou/PyTorch-Tutorial/tree/master/tutorial-contents">代码出处</a></p><p>这次学习了AutoEncode、DQN、GAN，均为经典网络。</p><a id="more"></a><h2 id="AutoEncode"><a href="#AutoEncode" class="headerlink" title="AutoEncode"></a>AutoEncode</h2><p>AutoEncode 可分为编码部分和解码部分，可以理解为先降维后还原来压缩数据的一种方式。<br>具体代码见上方链接。</p><h2 id="DQN"><a href="#DQN" class="headerlink" title="DQN"></a>DQN</h2><p>深度强化学习。Q-learning 的深度网络模型。根据reward做出选择。<br>具体代码见上方链接。</p><h2 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h2><p>对抗生成网络。Generator 会根据随机数来生成有意义的数据 , Discriminator 会学习如何判断哪些是真实数据 , 哪些是生成数据, 然后将学习的经验反向传递给 Generator, 让 Generator 能根据随机数生成更像真实数据的数据.<br>具体代码见上方链接。</p><h2 id="GPU加速"><a href="#GPU加速" class="headerlink" title="GPU加速"></a>GPU加速</h2><p>在网络，数据等变量后加上cuda函数</p><h2 id="dropout"><a href="#dropout" class="headerlink" title="dropout"></a>dropout</h2><p>减少过拟合，和传统机器学习中正则化有类似的作用</p><h2 id="批标准化"><a href="#批标准化" class="headerlink" title="批标准化"></a>批标准化</h2><p>Batch Normalization (BN) 被添加在每一个全连接和激励函数之间.使数据分布在激励函数的敏感作用区域。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://github.com/MorvanZhou/PyTorch-Tutorial/tree/master/tutorial-contents&quot;&gt;代码出处&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这次学习了AutoEncode、DQN、GAN，均为经典网络。&lt;/p&gt;</summary>
    
    
    
    <category term="pytorch学习笔记" scheme="http://yoursite.com/categories/pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>CNN,RNN网络搭建学习</title>
    <link href="http://yoursite.com/2020/03/08/CNN,RNN%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2020/03/08/CNN,RNN%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA%E5%AD%A6%E4%B9%A0/</id>
    <published>2020-03-08T03:15:45.000Z</published>
    <updated>2020-10-29T11:15:30.276Z</updated>
    
    <content type="html"><![CDATA[<p>代码来自于<a href="https://github.com/MorvanZhou/PyTorch-Tutorial/tree/master/tutorial-contents">这里</a></p><h2 id="提前划重点"><a href="#提前划重点" class="headerlink" title="提前划重点"></a>提前划重点</h2><h3 id="torchvision是独立于pytorch的关于图像操作的一些方便工具库"><a href="#torchvision是独立于pytorch的关于图像操作的一些方便工具库" class="headerlink" title="torchvision是独立于pytorch的关于图像操作的一些方便工具库"></a>torchvision是独立于pytorch的关于图像操作的一些方便工具库</h3><a id="more"></a><ul><li>vision.datasets : 几个常用视觉数据集，可以下载和加载，这里主要的高级用法就是可以看源码如何自己写自己的Dataset的子类</li><li>vision.models : 流行的模型，例如 AlexNet, VGG, ResNet 和 Densenet 以及 与训练好的参数。</li><li>vision.transforms : 常用的图像操作，例如：随机切割，旋转，数据类型转换，图像到tensor ,numpy 数组到tensor , tensor 到 图像等。</li><li>vision.utils : 用于把形似 (3 x H x W) 的张量保存到硬盘中，给一个mini-batch的图像可以产生一个图像格网。</li></ul><h3 id="以下CNN代码部分个人理解"><a href="#以下CNN代码部分个人理解" class="headerlink" title="以下CNN代码部分个人理解"></a>以下CNN代码部分个人理解</h3><pre><code class="python">train_data = torchvision.datasets.MNIST(    root=&#39;./mnist/&#39;,    train=True,                                     # this is training data    transform=torchvision.transforms.ToTensor(),    # Converts a PIL.Image or numpy.ndarray to                                                    # torch.FloatTensor of shape (C x H x W) and normalize in the range [0.0, 1.0]    download=DOWNLOAD_MNIST,)</code></pre><p>root参数是数据集本地路径<br>train为True时取训练集，False时取测试集<br>transform将原始数据转换为Tensor类型<br>download参数决定是否要下载数据集，如果有就不用下载了，设置为false</p><pre><code class="python">self.conv1 = nn.Sequential(         # input shape (1, 28, 28)            nn.Conv2d(                in_channels=1,              # input height                out_channels=16,            # n_filters                kernel_size=5,              # filter size                stride=1,                   # filter movement/step                padding=2,                  # if want same width and length of this image after Conv2d, padding=(kernel_size-1)/2 if stride=1            ),                              # output shape (16, 28, 28)            nn.ReLU(),                      # activation            nn.MaxPool2d(kernel_size=2),    # choose max value in 2x2 area, output shape (16, 14, 14)        )</code></pre><p>in_channels参数是height，在这里因为是灰度图所以通道数为1<br>out_channels为输出数通道数<br>stride步长<br>padding补零</p><pre><code class="python">test_output, _ = cnn(test_x[:10])pred_y = torch.max(test_output, 1)[1].data.numpy()print(pred_y, &#39;prediction number&#39;)print(test_y[:10].numpy(), &#39;real number&#39;)</code></pre><p>训练网络之后的测试</p><h3 id="以下RNN代码部分个人理解"><a href="#以下RNN代码部分个人理解" class="headerlink" title="以下RNN代码部分个人理解"></a>以下RNN代码部分个人理解</h3><p>rnn网络在第t时刻隐藏层的输出需要t-1时刻的隐藏层的输出，可以解决一些带有时序序列的问题<br>LSTM相比rnn更有利于解决长输出问题，即重点信息在最开始出现。有输入门，输出门，忘记门的概念</p><pre><code class="python">INPUT_SIZE = 28 # 图片宽度为28，每一个step读取一个宽度的像素点TIME_STEP = 28 # 图片为28 * 28, 每次读取28个像素点需要读取28个step</code></pre><pre><code class="python">class RNN(nn.Module):    def __init__(self):        super(RNN, self).__init__()        self.rnn = nn.RNN(            input_size=INPUT_SIZE,            hidden_size=32,     # rnn hidden unit            num_layers=1,       # number of rnn layer            batch_first=True,   # input &amp; output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)        )        self.out = nn.Linear(32, 1)    def forward(self, x, h_state):        # x (batch, time_step, input_size)        # h_state (n_layers, batch, hidden_size)        # r_out (batch, time_step, hidden_size)        r_out, h_state = self.rnn(x, h_state)        outs = []    # save all predictions        for time_step in range(r_out.size(1)):    # calculate output for each time step            outs.append(self.out(r_out[:, time_step, :]))        return torch.stack(outs, dim=1), h_state        # instead, for simplicity, you can replace above codes by follows        # r_out = r_out.view(-1, 32)        # outs = self.out(r_out)        # outs = outs.view(-1, TIME_STEP, 1)        # return outs, h_state        # or even simpler, since nn.Linear can accept inputs of any dimension        # and returns outputs with same dimension except for the last        # outs = self.out(r_out)        # return outs</code></pre><p>具体参数的意义可以看<a href="https://pytorch.org/docs/stable/nn.html#lstm">官网</a><br>在这里可以注意一下在处理图片时LSTM并没有用到之间的连接，返回值也是最后一个step的输入<br>而回归时用到了层与层之间的传递，初始化hstate为None.<br>注意在传递hstate时有一步hstate=hstate.data的操作<br>有一步用view函数reshape输入数据的操作</p><h2 id="CNN网络搭建（MNIST数据集）"><a href="#CNN网络搭建（MNIST数据集）" class="headerlink" title="CNN网络搭建（MNIST数据集）"></a>CNN网络搭建（MNIST数据集）</h2><p>代码</p><pre><code class="python"># standard libraryimport os# third-party libraryimport torchimport torch.nn as nnimport torch.utils.data as Dataimport torchvisionimport matplotlib.pyplot as plt# torch.manual_seed(1)    # reproducible# Hyper ParametersEPOCH = 1               # train the training data n times, to save time, we just train 1 epochBATCH_SIZE = 50LR = 0.001              # learning rateDOWNLOAD_MNIST = False# Mnist digits datasetif not(os.path.exists(&#39;./mnist/&#39;)) or not os.listdir(&#39;./mnist/&#39;):    # not mnist dir or mnist is empyt dir    DOWNLOAD_MNIST = Truetrain_data = torchvision.datasets.MNIST(    root=&#39;./mnist/&#39;,    train=True,                                     # this is training data    transform=torchvision.transforms.ToTensor(),    # Converts a PIL.Image or numpy.ndarray to                                                    # torch.FloatTensor of shape (C x H x W) and normalize in the range [0.0, 1.0]    download=DOWNLOAD_MNIST,)# plot one exampleprint(train_data.train_data.size())                 # (60000, 28, 28)print(train_data.train_labels.size())               # (60000)plt.imshow(train_data.train_data[0].numpy(), cmap=&#39;gray&#39;)plt.title(&#39;%i&#39; % train_data.train_labels[0])plt.show()# Data Loader for easy mini-batch return in training, the image batch shape will be (50, 1, 28, 28)train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)# pick 2000 samples to speed up testingtest_data = torchvision.datasets.MNIST(root=&#39;./mnist/&#39;, train=False)test_x = torch.unsqueeze(test_data.test_data, dim=1).type(torch.FloatTensor)[:2000]/255.   # shape from (2000, 28, 28) to (2000, 1, 28, 28), value in range(0,1)test_y = test_data.test_labels[:2000]class CNN(nn.Module):    def __init__(self):        super(CNN, self).__init__()        self.conv1 = nn.Sequential(         # input shape (1, 28, 28)            nn.Conv2d(                in_channels=1,              # input height                out_channels=16,            # n_filters                kernel_size=5,              # filter size                stride=1,                   # filter movement/step                padding=2,                  # if want same width and length of this image after Conv2d, padding=(kernel_size-1)/2 if stride=1            ),                              # output shape (16, 28, 28)            nn.ReLU(),                      # activation            nn.MaxPool2d(kernel_size=2),    # choose max value in 2x2 area, output shape (16, 14, 14)        )        self.conv2 = nn.Sequential(         # input shape (16, 14, 14)            nn.Conv2d(16, 32, 5, 1, 2),     # output shape (32, 14, 14)            nn.ReLU(),                      # activation            nn.MaxPool2d(2),                # output shape (32, 7, 7)        )        self.out = nn.Linear(32 * 7 * 7, 10)   # fully connected layer, output 10 classes    def forward(self, x):        x = self.conv1(x)        x = self.conv2(x)        x = x.view(x.size(0), -1)           # flatten the output of conv2 to (batch_size, 32 * 7 * 7)        output = self.out(x)        return output, x    # return x for visualizationcnn = CNN()print(cnn)  # net architectureoptimizer = torch.optim.Adam(cnn.parameters(), lr=LR)   # optimize all cnn parametersloss_func = nn.CrossEntropyLoss()                       # the target label is not one-hotted# following function (plot_with_labels) is for visualization, can be ignored if not interestedfrom matplotlib import cmtry: from sklearn.manifold import TSNE; HAS_SK = Trueexcept: HAS_SK = False; print(&#39;Please install sklearn for layer visualization&#39;)def plot_with_labels(lowDWeights, labels):    plt.cla()    X, Y = lowDWeights[:, 0], lowDWeights[:, 1]    for x, y, s in zip(X, Y, labels):        c = cm.rainbow(int(255 * s / 9)); plt.text(x, y, s, backgroundcolor=c, fontsize=9)    plt.xlim(X.min(), X.max()); plt.ylim(Y.min(), Y.max()); plt.title(&#39;Visualize last layer&#39;); plt.show(); plt.pause(0.01)plt.ion()# training and testingfor epoch in range(EPOCH):    for step, (b_x, b_y) in enumerate(train_loader):   # gives batch data, normalize x when iterate train_loader        output = cnn(b_x)[0]               # cnn output        loss = loss_func(output, b_y)   # cross entropy loss        optimizer.zero_grad()           # clear gradients for this training step        loss.backward()                 # backpropagation, compute gradients        optimizer.step()                # apply gradients        if step % 50 == 0:            test_output, last_layer = cnn(test_x)            pred_y = torch.max(test_output, 1)[1].data.numpy()            accuracy = float((pred_y == test_y.data.numpy()).astype(int).sum()) / float(test_y.size(0))            print(&#39;Epoch: &#39;, epoch, &#39;| train loss: %.4f&#39; % loss.data.numpy(), &#39;| test accuracy: %.2f&#39; % accuracy)            if HAS_SK:                # Visualization of trained flatten layer (T-SNE)                tsne = TSNE(perplexity=30, n_components=2, init=&#39;pca&#39;, n_iter=5000)                plot_only = 500                low_dim_embs = tsne.fit_transform(last_layer.data.numpy()[:plot_only, :])                labels = test_y.numpy()[:plot_only]                plot_with_labels(low_dim_embs, labels)plt.ioff()# print 10 predictions from test datatest_output, _ = cnn(test_x[:10])pred_y = torch.max(test_output, 1)[1].data.numpy()print(pred_y, &#39;prediction number&#39;)print(test_y[:10].numpy(), &#39;real number&#39;)</code></pre><h2 id="RNN分类网络（MNIST数据集）"><a href="#RNN分类网络（MNIST数据集）" class="headerlink" title="RNN分类网络（MNIST数据集）"></a>RNN分类网络（MNIST数据集）</h2><p>代码</p><pre><code class="python">import torchfrom torch import nnimport torchvision.datasets as dsetsimport torchvision.transforms as transformsimport matplotlib.pyplot as plt# torch.manual_seed(1)    # reproducible# Hyper ParametersEPOCH = 1               # train the training data n times, to save time, we just train 1 epochBATCH_SIZE = 64TIME_STEP = 28          # rnn time step / image heightINPUT_SIZE = 28         # rnn input size / image widthLR = 0.01               # learning rateDOWNLOAD_MNIST = True   # set to True if haven&#39;t download the data# Mnist digital datasettrain_data = dsets.MNIST(    root=&#39;./mnist/&#39;,    train=True,                         # this is training data    transform=transforms.ToTensor(),    # Converts a PIL.Image or numpy.ndarray to                                        # torch.FloatTensor of shape (C x H x W) and normalize in the range [0.0, 1.0]    download=DOWNLOAD_MNIST,            # download it if you don&#39;t have it)# plot one exampleprint(train_data.train_data.size())     # (60000, 28, 28)print(train_data.train_labels.size())   # (60000)plt.imshow(train_data.train_data[0].numpy(), cmap=&#39;gray&#39;)plt.title(&#39;%i&#39; % train_data.train_labels[0])plt.show()# Data Loader for easy mini-batch return in trainingtrain_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)# convert test data into Variable, pick 2000 samples to speed up testingtest_data = dsets.MNIST(root=&#39;./mnist/&#39;, train=False, transform=transforms.ToTensor())test_x = test_data.test_data.type(torch.FloatTensor)[:2000]/255.   # shape (2000, 28, 28) value in range(0,1)test_y = test_data.test_labels.numpy()[:2000]    # covert to numpy arrayclass RNN(nn.Module):    def __init__(self):        super(RNN, self).__init__()        self.rnn = nn.LSTM(         # if use nn.RNN(), it hardly learns            input_size=INPUT_SIZE,            hidden_size=64,         # rnn hidden unit            num_layers=1,           # number of rnn layer            batch_first=True,       # input &amp; output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)        )        self.out = nn.Linear(64, 10)    def forward(self, x):        # x shape (batch, time_step, input_size)        # r_out shape (batch, time_step, output_size)        # h_n shape (n_layers, batch, hidden_size)        # h_c shape (n_layers, batch, hidden_size)        r_out, (h_n, h_c) = self.rnn(x, None)   # None represents zero initial hidden state        # choose r_out at the last time step        out = self.out(r_out[:, -1, :])        return outrnn = RNN()print(rnn)optimizer = torch.optim.Adam(rnn.parameters(), lr=LR)   # optimize all cnn parametersloss_func = nn.CrossEntropyLoss()                       # the target label is not one-hotted# training and testingfor epoch in range(EPOCH):    for step, (b_x, b_y) in enumerate(train_loader):        # gives batch data        b_x = b_x.view(-1, 28, 28)              # reshape x to (batch, time_step, input_size)        output = rnn(b_x)                               # rnn output        loss = loss_func(output, b_y)                   # cross entropy loss        optimizer.zero_grad()                           # clear gradients for this training step        loss.backward()                                 # backpropagation, compute gradients        optimizer.step()                                # apply gradients        if step % 50 == 0:            test_output = rnn(test_x)                   # (samples, time_step, input_size)            pred_y = torch.max(test_output, 1)[1].data.numpy()            accuracy = float((pred_y == test_y).astype(int).sum()) / float(test_y.size)            print(&#39;Epoch: &#39;, epoch, &#39;| train loss: %.4f&#39; % loss.data.numpy(), &#39;| test accuracy: %.2f&#39; % accuracy)# print 10 predictions from test datatest_output = rnn(test_x[:10].view(-1, 28, 28))pred_y = torch.max(test_output, 1)[1].data.numpy()print(pred_y, &#39;prediction number&#39;)print(test_y[:10], &#39;real number&#39;)</code></pre><h2 id="RNN回归网络"><a href="#RNN回归网络" class="headerlink" title="RNN回归网络"></a>RNN回归网络</h2><p>代码</p><pre><code class="python">import torchfrom torch import nnimport numpy as npimport matplotlib.pyplot as plt# torch.manual_seed(1)    # reproducible# Hyper ParametersTIME_STEP = 10      # rnn time stepINPUT_SIZE = 1      # rnn input sizeLR = 0.02           # learning rate# show datasteps = np.linspace(0, np.pi*2, 100, dtype=np.float32)  # float32 for converting torch FloatTensorx_np = np.sin(steps)y_np = np.cos(steps)plt.plot(steps, y_np, &#39;r-&#39;, label=&#39;target (cos)&#39;)plt.plot(steps, x_np, &#39;b-&#39;, label=&#39;input (sin)&#39;)plt.legend(loc=&#39;best&#39;)plt.show()class RNN(nn.Module):    def __init__(self):        super(RNN, self).__init__()        self.rnn = nn.RNN(            input_size=INPUT_SIZE,            hidden_size=32,     # rnn hidden unit            num_layers=1,       # number of rnn layer            batch_first=True,   # input &amp; output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)        )        self.out = nn.Linear(32, 1)    def forward(self, x, h_state):        # x (batch, time_step, input_size)        # h_state (n_layers, batch, hidden_size)        # r_out (batch, time_step, hidden_size)        r_out, h_state = self.rnn(x, h_state)        outs = []    # save all predictions        for time_step in range(r_out.size(1)):    # calculate output for each time step            outs.append(self.out(r_out[:, time_step, :]))        return torch.stack(outs, dim=1), h_state        # instead, for simplicity, you can replace above codes by follows        # r_out = r_out.view(-1, 32)        # outs = self.out(r_out)        # outs = outs.view(-1, TIME_STEP, 1)        # return outs, h_state        # or even simpler, since nn.Linear can accept inputs of any dimension        # and returns outputs with same dimension except for the last        # outs = self.out(r_out)        # return outsrnn = RNN()print(rnn)optimizer = torch.optim.Adam(rnn.parameters(), lr=LR)   # optimize all cnn parametersloss_func = nn.MSELoss()h_state = None      # for initial hidden stateplt.figure(1, figsize=(12, 5))plt.ion()           # continuously plotfor step in range(100):    start, end = step * np.pi, (step+1)*np.pi   # time range    # use sin predicts cos    steps = np.linspace(start, end, TIME_STEP, dtype=np.float32, endpoint=False)  # float32 for converting torch FloatTensor    x_np = np.sin(steps)    y_np = np.cos(steps)    x = torch.from_numpy(x_np[np.newaxis, :, np.newaxis])    # shape (batch, time_step, input_size)    y = torch.from_numpy(y_np[np.newaxis, :, np.newaxis])    prediction, h_state = rnn(x, h_state)   # rnn output    # !! next step is important !!    h_state = h_state.data        # repack the hidden state, break the connection from last iteration    loss = loss_func(prediction, y)         # calculate loss    optimizer.zero_grad()                   # clear gradients for this training step    loss.backward()                         # backpropagation, compute gradients    optimizer.step()                        # apply gradients    # plotting    plt.plot(steps, y_np.flatten(), &#39;r-&#39;)    plt.plot(steps, prediction.data.numpy().flatten(), &#39;b-&#39;)    plt.draw(); plt.pause(0.05)plt.ioff()plt.show()</code></pre>]]></content>
    
    
    <summary type="html">&lt;p&gt;代码来自于&lt;a href=&quot;https://github.com/MorvanZhou/PyTorch-Tutorial/tree/master/tutorial-contents&quot;&gt;这里&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;提前划重点&quot;&gt;&lt;a href=&quot;#提前划重点&quot; class=&quot;headerlink&quot; title=&quot;提前划重点&quot;&gt;&lt;/a&gt;提前划重点&lt;/h2&gt;&lt;h3 id=&quot;torchvision是独立于pytorch的关于图像操作的一些方便工具库&quot;&gt;&lt;a href=&quot;#torchvision是独立于pytorch的关于图像操作的一些方便工具库&quot; class=&quot;headerlink&quot; title=&quot;torchvision是独立于pytorch的关于图像操作的一些方便工具库&quot;&gt;&lt;/a&gt;torchvision是独立于pytorch的关于图像操作的一些方便工具库&lt;/h3&gt;</summary>
    
    
    
    <category term="pytorch学习笔记" scheme="http://yoursite.com/categories/pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>学习使用Pytorch</title>
    <link href="http://yoursite.com/2020/03/07/%E5%AD%A6%E4%B9%A0%E4%BD%BF%E7%94%A8Pytorch/"/>
    <id>http://yoursite.com/2020/03/07/%E5%AD%A6%E4%B9%A0%E4%BD%BF%E7%94%A8Pytorch/</id>
    <published>2020-03-07T12:08:15.000Z</published>
    <updated>2020-10-29T11:15:30.276Z</updated>
    
    <content type="html"><![CDATA[<p>代码来自于<a href="https://github.com/MorvanZhou/PyTorch-Tutorial/tree/master/tutorial-contents">这里</a></p><h2 id="解惑"><a href="#解惑" class="headerlink" title="解惑"></a>解惑</h2><a id="more"></a><p><code>torch.manual_seed(1)    # reproducible</code><br>这句代码的意思是这样的，因为网络的参数是随机初始化的，所以将seed固定可以保证每次的结果固定</p><p><code>x = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1)  # x data (tensor), shape=(100, 1)</code><br>这句代码是说，要从-1 到1 的区间内取100个点（此时是一维），然后用unsqueeze函数升一维，至于在哪里升就要看那个dim参数的设定了，具体的看可以看<a href="https://blog.csdn.net/flysky_jay/article/details/81607289">这个博客</a></p><pre><code class="python">loss_func = torch.nn.MSELoss()  # this is for regression mean squared lossloss_func = torch.nn.CrossEntropyLoss()  # the target label is NOT an one-hotted</code></pre><p>这两个损失函数是常用的，第一个是均方差损失，常用于回归；第二个是交叉熵损失，常用于分类。简单清楚的描述可以看<a href="https://zhuanlan.zhihu.com/p/35709485">知乎大佬</a></p><pre><code class="python">n_data = torch.ones(100, 2)x0 = torch.normal(2*n_data, 1)      # class0 x data (tensor), shape=(100, 2)</code></pre><p>normal()函数第一个是你输入的数据，第二个是std标准差(不一定是一个数，也可以是和输入数据对应的tensor).</p><p><code>prediction = torch.max(out, 1)[1]</code><br>max()第一个参数是输入的数据，第二个是维度。函数返回两个tensor，第一个是value，第二个是index，此处用index来表示类别</p><pre><code class="python">for t in range(200):    prediction = net(x)     # input x and predict based on x    loss = loss_func(prediction, y)     # must be (1. nn output, 2. target)    optimizer.zero_grad()   # clear gradients for next train    loss.backward()         # backpropagation, compute gradients    optimizer.step()        # apply gradients</code></pre><p>这里注意注释</p><p>one-hotted 就是[0, 1, 0]这种只有1和0的,表示数据的所处的状态。</p><h2 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h2><pre><code class="python">import torchimport torch.nn.functional as Fimport matplotlib.pyplot as pltx = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1)  # x data (tensor), shape=(100, 1)y = x.pow(2) + 0.2*torch.rand(x.size())                 # noisy y data (tensor), shape=(100, 1)class Net(torch.nn.Module):    def __init__(self, n_feature, n_hidden, n_output):        super(Net, self).__init__()        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer        self.predict = torch.nn.Linear(n_hidden, n_output)   # output layer    def forward(self, x):        x = F.relu(self.hidden(x))      # activation function for hidden layer        x = self.predict(x)             # linear output        return xnet = Net(n_feature=1, n_hidden=10, n_output=1)     # define the networkprint(net)  # net architectureoptimizer = torch.optim.SGD(net.parameters(), lr=0.2)loss_func = torch.nn.MSELoss()  # this is for regression mean squared lossplt.ion()   # something about plottingfor t in range(200):    prediction = net(x)     # input x and predict based on x    loss = loss_func(prediction, y)     # must be (1. nn output, 2. target)    optimizer.zero_grad()   # clear gradients for next train    loss.backward()         # backpropagation, compute gradients    optimizer.step()        # apply gradients    if t % 5 == 0:        # plot and show learning process        plt.cla()        plt.scatter(x.data.numpy(), y.data.numpy())        plt.plot(x.data.numpy(), prediction.data.numpy(), &#39;r-&#39;, lw=5)        plt.text(0.5, 0, &#39;Loss=%.4f&#39; % loss.data.numpy(), fontdict=&#123;&#39;size&#39;: 20, &#39;color&#39;:  &#39;red&#39;&#125;)        plt.pause(0.1)plt.ioff()plt.show()</code></pre><p>好了，到这里已经看到了一个简单的回归网络</p><h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><pre><code class="python">import torchimport torch.nn.functional as Fimport matplotlib.pyplot as plt# torch.manual_seed(1)    # reproducible# make fake datan_data = torch.ones(100, 2)x0 = torch.normal(2*n_data, 1)      # class0 x data (tensor), shape=(100, 2)y0 = torch.zeros(100)               # class0 y data (tensor), shape=(100, 1)x1 = torch.normal(-2*n_data, 1)     # class1 x data (tensor), shape=(100, 2)y1 = torch.ones(100)                # class1 y data (tensor), shape=(100, 1)x = torch.cat((x0, x1), 0).type(torch.FloatTensor)  # shape (200, 2) FloatTensor = 32-bit floatingy = torch.cat((y0, y1), ).type(torch.LongTensor)    # shape (200,) LongTensor = 64-bit integerclass Net(torch.nn.Module):    def __init__(self, n_feature, n_hidden, n_output):        super(Net, self).__init__()        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer        self.out = torch.nn.Linear(n_hidden, n_output)   # output layer    def forward(self, x):        x = F.relu(self.hidden(x))      # activation function for hidden layer        x = self.out(x)        return xnet = Net(n_feature=2, n_hidden=10, n_output=2)     # define the networkprint(net)  # net architectureoptimizer = torch.optim.SGD(net.parameters(), lr=0.02)loss_func = torch.nn.CrossEntropyLoss()  # the target label is NOT an one-hottedplt.ion()   # something about plottingfor t in range(100):    out = net(x)                 # input x and predict based on x    loss = loss_func(out, y)     # must be (1. nn output, 2. target), the target label is NOT one-hotted    optimizer.zero_grad()   # clear gradients for next train    loss.backward()         # backpropagation, compute gradients    optimizer.step()        # apply gradients    if t % 2 == 0:        # plot and show learning process        plt.cla()        prediction = torch.max(out, 1)[1]        pred_y = prediction.data.numpy()        target_y = y.data.numpy()        plt.scatter(x.data.numpy()[:, 0], x.data.numpy()[:, 1], c=pred_y, s=100, lw=0, cmap=&#39;RdYlGn&#39;)        accuracy = float((pred_y == target_y).astype(int).sum()) / float(target_y.size)        plt.text(1.5, -4, &#39;Accuracy=%.2f&#39; % accuracy, fontdict=&#123;&#39;size&#39;: 20, &#39;color&#39;:  &#39;red&#39;&#125;)        plt.pause(0.1)plt.ioff()plt.show()</code></pre><p>好了，到这里我们也见过了简单的分类网络。</p><h2 id="快速搭建网络"><a href="#快速搭建网络" class="headerlink" title="快速搭建网络"></a>快速搭建网络</h2><p>除了上面的像这样的搭建方法</p><pre><code class="python">class Net(torch.nn.Module):    def __init__(self, n_feature, n_hidden, n_output):        super(Net, self).__init__()        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer        self.predict = torch.nn.Linear(n_hidden, n_output)   # output layer    def forward(self, x):        x = F.relu(self.hidden(x))      # activation function for hidden layer        x = self.predict(x)             # linear output        return xnet1 = Net(1, 10, 1)</code></pre><p>还可以像这样</p><pre><code class="python">net2 = torch.nn.Sequential(    torch.nn.Linear(1, 10),    torch.nn.ReLU(),    torch.nn.Linear(10, 1))</code></pre><h2 id="保存提取"><a href="#保存提取" class="headerlink" title="保存提取"></a>保存提取</h2><p>这里有两种保存方式，一种是保存网络， 一种是只保存参数</p><pre><code class="python">import torchimport matplotlib.pyplot as plt# fake datax = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1)  # x data (tensor), shape=(100, 1)y = x.pow(2) + 0.2*torch.rand(x.size())  # noisy y data (tensor), shape=(100, 1)def save():    # save net1    net1 = torch.nn.Sequential(        torch.nn.Linear(1, 10),        torch.nn.ReLU(),        torch.nn.Linear(10, 1)    )    optimizer = torch.optim.SGD(net1.parameters(), lr=0.5)    loss_func = torch.nn.MSELoss()    for t in range(100):        prediction = net1(x)        loss = loss_func(prediction, y)        optimizer.zero_grad()        loss.backward()        optimizer.step()    # plot result    plt.figure(1, figsize=(10, 3))    plt.subplot(131)    plt.title(&#39;Net1&#39;)    plt.scatter(x.data.numpy(), y.data.numpy())    plt.plot(x.data.numpy(), prediction.data.numpy(), &#39;r-&#39;, lw=5)    # 2 ways to save the net    torch.save(net1, &#39;net.pkl&#39;)  # save entire net    torch.save(net1.state_dict(), &#39;net_params.pkl&#39;)   # save only the parametersdef restore_net():    # restore entire net1 to net2    net2 = torch.load(&#39;net.pkl&#39;)    prediction = net2(x)    # plot result    plt.subplot(132)    plt.title(&#39;Net2&#39;)    plt.scatter(x.data.numpy(), y.data.numpy())    plt.plot(x.data.numpy(), prediction.data.numpy(), &#39;r-&#39;, lw=5)def restore_params():    # restore only the parameters in net1 to net3    net3 = torch.nn.Sequential(        torch.nn.Linear(1, 10),        torch.nn.ReLU(),        torch.nn.Linear(10, 1)    )    # copy net1&#39;s parameters into net3    net3.load_state_dict(torch.load(&#39;net_params.pkl&#39;))    prediction = net3(x)    # plot result    plt.subplot(133)    plt.title(&#39;Net3&#39;)    plt.scatter(x.data.numpy(), y.data.numpy())    plt.plot(x.data.numpy(), prediction.data.numpy(), &#39;r-&#39;, lw=5)    plt.show()# save net1save()# restore entire net (may slow)restore_net()# restore only the net parametersrestore_params()</code></pre><h2 id="批训练"><a href="#批训练" class="headerlink" title="批训练"></a>批训练</h2><p>数据量很大时我们需要分批次进行训练</p><pre><code class="python">import torchimport torch.utils.data as Datatorch.manual_seed(1)    # reproducibleBATCH_SIZE = 5# BATCH_SIZE = 8x = torch.linspace(1, 10, 10)       # this is x data (torch tensor)y = torch.linspace(10, 1, 10)       # this is y data (torch tensor)torch_dataset = Data.TensorDataset(x, y)loader = Data.DataLoader(    dataset=torch_dataset,      # torch TensorDataset format    batch_size=BATCH_SIZE,      # mini batch size    shuffle=True,               # random shuffle for training    num_workers=2,              # subprocesses for loading data)def show_batch():    for epoch in range(3):   # train entire dataset 3 times        for step, (batch_x, batch_y) in enumerate(loader):  # for each training step            # train your data...            print(&#39;Epoch: &#39;, epoch, &#39;| Step: &#39;, step, &#39;| batch x: &#39;,                  batch_x.numpy(), &#39;| batch y: &#39;, batch_y.numpy())if __name__ == &#39;__main__&#39;:    show_batch()</code></pre><h2 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h2><p>SGD只是一个很简单的方法（相对于大佬）， 处理数据时会有点慢，所以要学习叼的方法</p><pre><code class="python">import torchimport torch.utils.data as Dataimport torch.nn.functional as Fimport matplotlib.pyplot as plt# torch.manual_seed(1)    # reproducibleLR = 0.01BATCH_SIZE = 32EPOCH = 12# fake datasetx = torch.unsqueeze(torch.linspace(-1, 1, 1000), dim=1)y = x.pow(2) + 0.1*torch.normal(torch.zeros(*x.size()))# plot datasetplt.scatter(x.numpy(), y.numpy())plt.show()# put dateset into torch datasettorch_dataset = Data.TensorDataset(x, y)loader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2,)# default networkclass Net(torch.nn.Module):    def __init__(self):        super(Net, self).__init__()        self.hidden = torch.nn.Linear(1, 20)   # hidden layer        self.predict = torch.nn.Linear(20, 1)   # output layer    def forward(self, x):        x = F.relu(self.hidden(x))      # activation function for hidden layer        x = self.predict(x)             # linear output        return xif __name__ == &#39;__main__&#39;:    # different nets    net_SGD         = Net()    net_Momentum    = Net()    net_RMSprop     = Net()    net_Adam        = Net()    nets = [net_SGD, net_Momentum, net_RMSprop, net_Adam]    # different optimizers    opt_SGD         = torch.optim.SGD(net_SGD.parameters(), lr=LR)    opt_Momentum    = torch.optim.SGD(net_Momentum.parameters(), lr=LR, momentum=0.8)    opt_RMSprop     = torch.optim.RMSprop(net_RMSprop.parameters(), lr=LR, alpha=0.9)    opt_Adam        = torch.optim.Adam(net_Adam.parameters(), lr=LR, betas=(0.9, 0.99))    optimizers = [opt_SGD, opt_Momentum, opt_RMSprop, opt_Adam]    loss_func = torch.nn.MSELoss()    losses_his = [[], [], [], []]   # record loss    # training    for epoch in range(EPOCH):        print(&#39;Epoch: &#39;, epoch)        for step, (b_x, b_y) in enumerate(loader):          # for each training step            for net, opt, l_his in zip(nets, optimizers, losses_his):                output = net(b_x)              # get output for every net                loss = loss_func(output, b_y)  # compute loss for every net                opt.zero_grad()                # clear gradients for next train                loss.backward()                # backpropagation, compute gradients                opt.step()                     # apply gradients                l_his.append(loss.data.numpy())     # loss recoder    labels = [&#39;SGD&#39;, &#39;Momentum&#39;, &#39;RMSprop&#39;, &#39;Adam&#39;]    for i, l_his in enumerate(losses_his):        plt.plot(l_his, label=labels[i])    plt.legend(loc=&#39;best&#39;)    plt.xlabel(&#39;Steps&#39;)    plt.ylabel(&#39;Loss&#39;)    plt.ylim((0, 0.2))    plt.show()    ```    好了，到这里你已经学习了使用Pytorch的基本方法流程了。</code></pre>]]></content>
    
    
    <summary type="html">&lt;p&gt;代码来自于&lt;a href=&quot;https://github.com/MorvanZhou/PyTorch-Tutorial/tree/master/tutorial-contents&quot;&gt;这里&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;解惑&quot;&gt;&lt;a href=&quot;#解惑&quot; class=&quot;headerlink&quot; title=&quot;解惑&quot;&gt;&lt;/a&gt;解惑&lt;/h2&gt;</summary>
    
    
    
    <category term="pytorch学习笔记" scheme="http://yoursite.com/categories/pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>Hello Pytorch</title>
    <link href="http://yoursite.com/2020/03/06/Hello-Pytorch/"/>
    <id>http://yoursite.com/2020/03/06/Hello-Pytorch/</id>
    <published>2020-03-06T07:53:40.000Z</published>
    <updated>2020-10-29T11:15:30.276Z</updated>
    
    <content type="html"><![CDATA[<h2 id="安装Pytorch"><a href="#安装Pytorch" class="headerlink" title="安装Pytorch"></a>安装Pytorch</h2><p>跟大佬学习用anaconda，创建虚拟环境，然后参照pytorch官网教程安装。<br>遇到了torch和numpy版本冲突的问题，卸载numpy安一个合适的版本</p><a id="more"></a><h2 id="简单使用Pytorch"><a href="#简单使用Pytorch" class="headerlink" title="简单使用Pytorch"></a>简单使用Pytorch</h2><p><code>asdf</code></p><pre><code class="python3">// tensor 的使用torch.tensor(data, requires_grad=True)// 与numpy的array之间相互转换torch.from_numpy(np_array)torch.numpy(tensor)</code></pre><p>更多详细API见<a href="https://pytorch.org/docs/stable/torch.html">官方</a></p><pre><code class="python">// 激励函数的使用import torch.nn.functional as FF.relu(tensor)</code></pre><p>更多详细API见<a href="https://pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions">官方</a></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;安装Pytorch&quot;&gt;&lt;a href=&quot;#安装Pytorch&quot; class=&quot;headerlink&quot; title=&quot;安装Pytorch&quot;&gt;&lt;/a&gt;安装Pytorch&lt;/h2&gt;&lt;p&gt;跟大佬学习用anaconda，创建虚拟环境，然后参照pytorch官网教程安装。&lt;br&gt;遇到了torch和numpy版本冲突的问题，卸载numpy安一个合适的版本&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="pytorch学习笔记" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2020/03/05/hello-world/"/>
    <id>http://yoursite.com/2020/03/05/hello-world/</id>
    <published>2020-03-05T03:15:45.000Z</published>
    <updated>2020-10-29T11:15:30.276Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre><code class="bash">$ hexo new &quot;My New Post&quot;</code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre><code class="bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre><code class="bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre><code class="bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    <category term="语法" scheme="http://yoursite.com/categories/%E8%AF%AD%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>markdown syntax</title>
    <link href="http://yoursite.com/2020/03/03/markdown-syntax/"/>
    <id>http://yoursite.com/2020/03/03/markdown-syntax/</id>
    <published>2020-03-03T12:17:35.000Z</published>
    <updated>2020-10-29T11:15:30.276Z</updated>
    
    <content type="html"><![CDATA[<p>$A$</p><pre><code class="js">function add(n1, n2) &#123;  const a1 = n1.split(&#39;&#39;).reverse();  const a2 = n2.split(&#39;&#39;).reverse();  const result = [];  for (let i = 0, l = Math.max(a1.length, a2.length); i &lt; l; i++) &#123;    result[i] = (result[i] || 0) + parseInt(a1[i] || 0) + parseInt(a2[i] || 0);    while (result[i] &gt;= 10) &#123;      result[i] -= 10;      result[i + 1] = (result[i + 1] || 0) + 1;    &#125;  &#125;  return result.reverse().join(&#39;&#39;);&#125;</code></pre><h1 id="一级标题"><a href="#一级标题" class="headerlink" title="一级标题"></a>一级标题</h1><h2 id="二级标题"><a href="#二级标题" class="headerlink" title="二级标题"></a>二级标题</h2><h3 id="三级标题"><a href="#三级标题" class="headerlink" title="三级标题"></a>三级标题</h3><h4 id="四级标题"><a href="#四级标题" class="headerlink" title="四级标题"></a>四级标题</h4><h5 id="五级标题"><a href="#五级标题" class="headerlink" title="五级标题"></a>五级标题</h5><h6 id="六级标题"><a href="#六级标题" class="headerlink" title="六级标题"></a>六级标题</h6><p><strong>加粗</strong><br><em>倾斜</em><br><strong><em>加粗倾斜</em></strong><br><del>加删除线</del></p><blockquote><p>引用的内容（可以嵌套）</p></blockquote><h2 id="分割线"><a href="#分割线" class="headerlink" title="分割线"></a>分割线</h2><hr><p><img src="http://file.koolearn.com/20161207/14810957953513.png" alt="图片 alt" title="网上随便找的"></p><p><a href="超链接地址" title="超链接title">超链接名</a></p><ul><li>列表内容</li></ul><ul><li>列表内容</li></ul><ul><li>列表内容</li></ul><ol><li>列表内容</li><li>列表内容</li><li>列表内容</li></ol><table><thead><tr><th>表头</th><th align="center">表头</th><th align="right">表头</th></tr></thead><tbody><tr><td>内容</td><td align="center">内容</td><td align="right">内容</td></tr><tr><td>内容</td><td align="center">内容</td><td align="right">内容</td></tr></tbody></table><pre><code>第二行分割表头和内容-有一个就行文字默认居左--两边加:表示文字居中--右边加:表示文字居右原生的语法两边都要用|包起来，此处原作者省略。</code></pre><p>单行代码:代码之间分别用一个反引号包起来<br><code>代码内容</code></p><p>代码块:代码之间分别用三个反引号包起来，且两边的反引号独占一行</p><pre><code class="flow"> st=&gt;start: 开始  op=&gt;operation: My Operation  cond=&gt;condition: Yes or No?  e=&gt;end: 结束  st-&gt;op-&gt;cond  cond(yes)-&gt;e  cond(no)-&gt;op </code></pre><pre><code class="flow">st=&gt;start: 开始框op=&gt;operation: 处理框cond=&gt;condition: 判断框(是或否?)sub1=&gt;subroutine: 子流程io=&gt;inputoutput: 输入输出框e=&gt;end: 结束框st-&gt;op-&gt;condcond(yes)-&gt;io-&gt;econd(no)-&gt;sub1(right)-&gt;op</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;$A$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;js&quot;&gt;function add(n1, n2) &amp;#123;
  const a1 = n1.split(&amp;#39;&amp;#39;).reverse();
  const a2 = n2.split(&amp;#39;&amp;#39;).</summary>
      
    
    
    
    <category term="语法" scheme="http://yoursite.com/categories/%E8%AF%AD%E6%B3%95/"/>
    
    
  </entry>
  
</feed>
