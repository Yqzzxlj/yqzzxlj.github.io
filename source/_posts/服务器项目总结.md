---
title: 服务器项目总结
date: 2021-03-05 22:15:36
tags:
    - c++
    - HTTP
    - 项目
categories:
    - 项目相关
---
# Tiny Web Server 服务器项目整理总结

### 1. Reactor模型

主线程（I/O处理单元）只负责监听文件描述上是否有事件发生，有的话就立即将该事件通知工作线程（逻辑单元）。除此之外，主线程不做任何其他实质性的工作。读写数据，接受新的连接，以及处理新的连接，以及处理客户请求均在工作线程中完成。

1. 主线程往epoll内核事件表中注册socket上的读就绪事件。
2. 主线程调用epoll_wait等待socket上有数据可读。
3. 当socket上有数据可读时，epoll_wait通知主线程。主线程则将socket可读事件放入请求队列。
4. 睡眠在请求队列上的某个工作线程被唤醒，他从socket读取数据并处理客户请求，然后往epoll内核事件表中注册该socket上的写就绪事件。
5. 主线程调用epoll_wait()等待socket可写。
6. 当socket可写时，epoll_wait通知主线程。主线程将socket可写事件放入请求队列。
7. 睡眠在请求队列上的某个工作线程被唤醒，它往socket上写入服务器处理客户请求的结果。

### 2. 线程池

+ 资源重用，避免了线程频繁建立、关闭的开销
+ 控制资源的使用。如果不使用池，每次都需要创建一个线程，这样系统的稳定性受系统连接需求影响很大，很容易产生资源浪费和高负载异常。池能够使性能最大化，将资源利用控制在一定的水平之下。连接池能控制池中的连接数量，增强了系统在大量用户应用时的稳定性。
+ 实现：构造、析构、任务enqueue

```c++
#pragma once
#include <vector>
#include <queue>

#include <mutex>
#include <condition_variable>
#include <thread>
#include <functional>
#include <future>
#include <memory>
#include <stdexcept>

class ThreadPool {
public:
  ThreadPool(size_t);
  ~ThreadPool();

  template<typename F, typename... Args>
  auto enqueue(F&&f, Args&&... args)
    -> std::future<typename std::result_of<F(Args...)>::type>;

private:
  std::vector<std::thread> workers;
  std::queue<std::function<void()>> tasks;

  std::mutex queue_mutex;
  std::condition_variable condition;
  bool stop;
};

inline ThreadPool::ThreadPool(size_t num_threads) : stop(false) {
  for (size_t i = 0; i < num_threads; ++i) {
    workers.emplace_back(
      [this](){
        while(true) {
          std::function<void()> task;
          {
            std::unique_lock<std::mutex> lock(this->queue_mutex);
            this->condition.wait(lock,
              [this](){return this->stop || !this->tasks.empty();});
            if (this->stop && this->tasks.empty()) {
              return;
            }
            task = std::move(this->tasks.front());
            this->tasks.pop();
          }
          task();
        }
      }
    );
  }
}

inline ThreadPool::~ThreadPool() {
  {
    std::unique_lock<std::mutex> lock(queue_mutex);
    stop = true;
  }
  condition.notify_all();
  for (std::thread& worker : workers) {
    worker.join();
  }
}

template<typename F, typename... Args>
auto ThreadPool::enqueue(F&& f, Args&&... args)
  -> std::future<typename std::result_of<F(Args...)>::type> {

  using return_type = typename std::result_of<F(Args...)>::type;

  auto task = std::make_shared<std::packaged_task<return_type()>> (
    std::bind(std::forward<F>(f), std::forward<Args>(args)...)
  );

  std::future<return_type> res = task->get_future();
  {
    std::unique_lock<std::mutex> lock(queue_mutex);
    if (stop) {
      throw std::runtime_error("enqueue on stopped ThreadPool");
    }
    tasks.emplace([task](){(*task)();});
  }
  condition.notify_one();
  return res;
}
```

### 3. 状态机解析HTTP请求

LINE_STATE， PARSE_STATE, HTTP_CODE

```c++
	enum LINE_STATE {LINE_OK = 0, LINE_BAD, LINE_MORE};
  enum PARSE_STATE {PARSE_REQUESTLINE = 0, PARSE_HEADER, PARSE_BODY};
  enum HTTP_CODE {NO_REQUEST = 0, GET_REQUEST, BAD_REQUEST, NOT_IMPLEMNTED, 
                  FORBIDDEN_REQUEST, INTERNAL_ERROR, CLOSED_CONNECTION};

  static LINE_STATE parse_line(Buffer& buffer);
  static HTTP_CODE parse_request_line(const std::string&, PARSE_STATE&, HttpRequest&);
  static HTTP_CODE parse_headers(const std::string&, PARSE_STATE&, HttpRequest&);
  static HTTP_CODE parse_body(const std::string&, HttpRequest&);
  static HTTP_CODE parse_content(Buffer& buffer, PARSE_STATE &parse_state, HttpRequest& request);
```

### 4. 定时器结构

最小堆实现。

每一个TimerNode包含一个expired Time，和Httpdata的指针，

有一个_deleted字段支持惰性删除。

```c++
void TimerManager::handleExpiredEvent() {
  std::unique_lock<std::mutex> lock;
  TimerNode::current_time();
  LOG_DEBUG << "handing expired event";
  while (!timer_queue_.empty()) {
    TimerNodePtr timer_node = timer_queue_.top();
    if (timer_node->is_deleted()) {
      timer_queue_.pop();
    } else if (timer_node->is_expire()){
      timer_queue_.pop();
    } else {
      break;
    }
  }
}
```



#### 实现定时器的其他方式

升序链表、时间轮

### 5. 双缓冲区技术实现异步日志系统

#### 为什么要一个日志系统？

 记录系统状态，方便查找出错原因

#### 为什么要是实现一个日志系统？

stdio在使用上不便，容易出现缓冲区溢出、格式字符串混淆等错误，而且很难保证类型安全。不可扩展。

iostream是可扩展的类型安全的IO机制。但是ostream格式化输出繁琐，istream不适合输入带格式的数据，线程不安全，不适合在多线程程序中做logging。

应该使用成熟的多线程库。

#### 双缓冲区？

准备两块buffer，A和B，前端负责往bufferA填数据，后端负责将bufferB的数据写入文件。当bufferA写满之后，交换A和B，让后端将bufferA的数据写入文件，而前端则往bufferB填入新的日志消息，如此往复。

好处是在新建日志消息的时候不必等待磁盘文件操作，也避免每条新日志都触发（唤醒）后端日志线程。换言之，前端不是将一条条日志消息分别传给后端，而是将多条日志消息拼成一个大的buffer传给后端，相当于批处理，减少了线程唤醒的品读，降低开销。

#### 实现

用一个线程（日志线程）负责收集日志消息并写入文件，其他业务线程只管往这个“日志线程”发送消息，这称为异步日志。

多生产者-单消费者问题，对于生产者而言，要尽量做到低延迟，低cpu开销，无阻塞；对消费者而言要做到足够大的吞吐量，并占用较少资源。

+ FileUtil 封装了文件的打开，关闭，写入。
+ LogFile 在FileUtil的基础上，加了锁，和按写次数flush。
+ LogStream 重载operator<<,将内容存在自己的缓存中。
+ AsyncLogging 启动一个log线程，负责定时或缓冲区填满时，将log写入logFile。双缓冲交换指针。
+ Logging 是对外接口，内含一个LogStream，将内容输入到LogStream的buffer中，并有一个静态的AsyncLogger， 向日志文件内写内容。实现了日志等级,格式化

```c++
void AsyncLogging::threadFunc() {
  assert(running_);
  LogFile output(basename_);
 	// bufferB
  BufferPtr newBuffer1(new Buffer());
  BufferPtr newBuffer2(new Buffer());
  newBuffer1->bzero();
  newBuffer2->bzero();
  BufferVector buffersToWrite;
  buffersToWrite.reserve(16);
  while (running_) {
    // 确保bufferB为空
    assert(newBuffer1 != nullptr && newBuffer1->length() == 0);
    assert(newBuffer2 != nullptr && newBuffer2->length() == 0);
    assert(buffersToWrite.empty());
		
    // 将写有数据的bufferA和为空的bufferB交换
    {
      std::unique_lock<std::mutex> lock(mutex_);
      // bufferA为空时，等待woker线程写入数据
      if (buffers_.empty()) {
        std::chrono::seconds dura(flushInterval_);
        cond_.wait_for(lock, dura);
      }
      // 将bufferA中所有数据放入vector中
      buffers_.push_back(std::move(currentBuffer_));
      // 将空的newBuffer1换给currentBuffer_
      currentBuffer_ = std::move(newBuffer1);
      // bufferA和bufferB交换
      buffersToWrite.swap(buffers_);
      // 将空的newBuffer2换给nextBuffer_(备用)
      if (nextBuffer_ == nullptr) {
        nextBuffer_ = std::move(newBuffer2);
      }
    }

		// 取保bufferB不为空
    assert(!buffersToWrite.empty());
		
    // bufferB内容过多时，认为woker线程出现问题，丢弃多余数据
    if(buffersToWrite.size() > 25) {
      buffersToWrite.erase(buffersToWrite.begin() + 2, buffersToWrite.end());
    }
		
    // 将bufferB中内容写入文件
    for (const auto& buffer : buffersToWrite) {
      output.append(buffer->data(), buffer->length());
    }

    if (buffersToWrite.size() > 2) {
      buffersToWrite.resize(2);
    }
		
    // refill newBuffer2
    if (newBuffer1 == nullptr) {
      assert(!buffersToWrite.empty());
      newBuffer1 = std::move(buffersToWrite.back());
      buffersToWrite.pop_back();
      newBuffer1->reset();
    }
		// refill newBuffer2
    if (newBuffer2 == nullptr) {
      assert(!buffersToWrite.empty());
      newBuffer2 = std::move(buffersToWrite.back());
      buffersToWrite.pop_back();
      newBuffer2->reset();
    }
		
    buffersToWrite.clear();
    output.flush();
  }
  output.flush();
}
```



#### 其他方法

+ 用全局锁保护IO，全部线程抢一个锁
+ 每个线程单独写一个日志文件，业务线程阻塞在写磁盘操作上。

### 6. RAII机制

资源获得即初始化。

智能指针管理socket，fd等资源、unique_lock管理mutex

为什么会用到RAII？

没用智能指针的时候会出现栈中的Socket自动析构，多次关闭fd的情况。

### 7. 压测

Webbench。4内核、4G内存。1000clients，60s，QPS 36621

内部实现有两种机制，一种是等待结果计算再开始下一次请求。另一种不等待计算结果直接开始下一次请求

会问到HTTP管线化（HTTP pipelining）

进程间通信(pipe)：

管道：如果没有缓冲区，单纯的往其中放入元素立马就会进入阻塞状态，必须有其他的线程从其中取走元素。通俗的讲要有一个线程不断的取这个管道的元素，才能往其中放入元素。它就像一个窄窄的门框，进去就得出来。

而有一个缓冲区的管道想一段地道，放入的元素不会马上进入阻塞状态，只有第二个准备进入而第一个还没有进入的情况下才会阻塞。

### 8. 一个请求到来的处理过程

启动：服务器listen_fd, 新建线程池，和epoll_fd。 令epoll监听listen_fd(不要注册EPOLLONESHOT）.

请求到来：connect，epoll通知事件就绪，将client_socket设置非阻塞，Nodelay（禁用nagle算法），将fd和httpData绑定，然后重新加入epoll，添加定时器。将处理连接请求的函数推入线程池的任务队列。

处理请求：

+ 从socket中读取数据，逐步分析请求行（方法，url，版本）首部行，空行，实体主体。
+ 如果是HEAD请求，发送头部，如果是GET，除了头部之外在发送指定文件。（共享内存，mmap，munmap）
+ 如果是短链接则直接关闭，长连接重新添加定时器，再次令epoll监听fd

### 9 遇到的问题

1. webbench测试完就会宕机。

连接建立，若某一端关闭连接，而另一端仍然向它写数据，第一次写数据后会收到RST响应，此后再写数据，内核将向进程发出SIGPIPE信号，通知进程此连接已经断开。而SIGPIPE信号的默认处理是终止程序，导致上述问题的发生。应该忽略SIGPIPE信号。

2. 头文件互相包含

   前向声明。

3. 

### 10. 为什么用非阻塞IO

- 水平触发：只要缓冲区有数据准备好就传递就绪信号
- 边缘触发：只有新数据到来才会传递就绪信号，没有新数据到来时尽管有旧数据没有呗读取也不通知

ET模式在很大程度上减少了epoll事件被重复触发的次数，因此效率要比LT模式高。epoll工作在ET模式的时候，必须使用非阻塞套接口，以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。

### 11. 无锁队列

CAS

### 12. epoll

epoll是select和poll的增强版本。相对于select和poll来说，epoll更加的灵活，没有描述符的限制。epoll使用一个文件描述符管理多个描述符，将用户关系的文件描述符放到内核一个事件表中，这样用户空间和内核空间的数据拷贝只需要一次。

```c++
#include <sys/epoll.h>
int epoll_create(int size);
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
int epoll_wait(int epfd, struct epoll_event* events, int maxevents, int timeout);
```

epoll_create 创建一个epoll句柄，size用来告诉内核监听的数目。这个不同于select中的第一个参数。需要注意的是，创建好epoll句柄后，他会占用一个fd值，所以使用完后要关闭

epoll_ctl 事件注册函数，不同于select() 监听事件时告诉内核监听什么样的事件， 而是先注册监听事件的类型。第一个参数是epoll_create()的返回值，第二个参数表示动作。（CTL的增删改）第三个参数是需要监听的参数，第四个参数告诉内核需要监听什么事。events是几个宏 的集合（类似poll）

- 水平触发：只要缓冲区有数据准备好就传递就绪信号
- 边缘触发：只有新数据到来才会传递就绪信号，没有新数据到来时尽管有旧数据没有被读取也不通知

ET模式在很大程度上减少了epoll事件被重复触发的次数，因此效率要比LT模式高。epoll工作在ET模式的时候，必须使用非阻塞套接口，以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。

[epoll底层实现过程](https://blog.csdn.net/qq_29108585/article/details/60468522)

首先epoll_create创建一个epoll文件描述符，底层同时创建一个红黑树，和一个就绪链表；红黑树存储所监控的文件描述符的节点数据，就绪链表存储就绪的文件描述符的节点数据；epoll_ctl将会添加新的描述符，首先判断是红黑树上是否有此文件描述符节点，如果有，则立即返回。如果没有， 则在树干上插入新的节点，并且告知内核注册回调函数。当接收到某个文件描述符过来数据时，那么内核将该节点插入到就绪链表里面。epoll_wait将会接收到消息，并且将数据拷贝到用户空间，清空链表。对于LT模式epoll_wait清空就绪链表之后会检查该文件描述符是哪一种模式，如果为LT模式，且必须该节点确实有事件未处理，那么就会把该节点重新放入到刚刚删除掉的且刚准备好的就绪链表，epoll_wait马上返回。ＥＴ模式不会检查，只会调用一次

#### EPOLLONESHOT事件

即使使用ET模式，一个socket上的某个事件还是可能被多次触发。比如，一个线程在读取完某个socket上的数据开始处理这些数据，而在数据的处理过程中该socket上又有新数据可读（EPOLLIN再次被触发）此时另外一个线程被唤醒来读取这些新的数据。于是出现了两个线程同时擦耦走一个socket的局面。我们希望**一个socket连接在任意时刻都只被一个线程处理**。这时要使用epoll的EPOLLONESHOT事件实现。

对于注册了POLLONESHOT事件的文件描述符，系统最多触发一个事件，且只触发一次，**除非用epoll_ctl函数重置**。当一个线程在处理某个socket时，其他线程是不可能有机会操作该socket的。操作完之后应重置EPOLLONESHOT事件。如果再处理时有新来了请求该线程将继续为这个socket服务。

监听socket listen_fd 上是不能注册EPOLLONESHOT事件的，否则应用程序只能处理一个客户连接。

后续的客户连接请求将不再触发listenfd上的EPOLLIN事件



#### select poll epoll区别

- select

1. select函数使用整形变量保存当前管理的用户进程数，使用句柄指针向可读进程集合、可写进程集合以及错误进程集合，使用毫秒级定时器结构体进程轮询计时。

- poll

1. 使用事件代码代替select可读可写错误指针，能够监听更多种事件，将输入输出事件分开，下次进入时不必重新初始化
2. 计时器采用整形变量，计时精度变低，只能执行秒级计时。
3. 可监控的数据量远大于select。

- epoll

1. 定时器超时后不必全局扫描管理的进程，epoll函数只对活跃的socket进行操作，每个socket的数据准备好后会调用回调函数使socket变为活跃。
2. 使用mmap共享内存加速内核和用户空间消息传递

### 13. 多进程和多线程的适用场景

1. 需要频繁创建和销毁的优先使用线程
2. 强相关的处理使用线程，弱相关使用进程
3. 可能扩展到多机分布的使用进程，多核分布的使用线程

### 14. Buffer

#### 14.1 buffer的必要性

non-blocking IO的核心思想是避免阻塞在read()或write()或其他IO系统调用上这样可以最大限度地服用thread-of-control，让一个线程能够服务于多个socket连接。IO线程只能阻塞在IO multiplexing函数上。

需要有InputBuffer和OutputBuffer

#### 14.2 Buffer的功能需求

Buffer像是一个queue，从末尾写数据，从头部读数据。

+ 对外表现为一块连续的内存
+ size()可以自动增长，以适应不同大小的消息
+ 内部以std::vector<char>来保存数据，并提供相应的访问函数。

#### 14.3 readFd()

```c++
ssize_t Buffer::readFd(int fd, int* saved_errno) {
  char extrabuf[65536];
  struct iovec vec[2];
  const size_t writeable = writable_bytes();

  vec[0].iov_base = begin() + write_index;
  vec[0].iov_len = writeable;
  vec[1].iov_base = extrabuf;
  vec[1].iov_len = sizeof(extrabuf);

  const int iovcnt = (writeable < sizeof (extrabuf) ? 2 : 1);
  const ssize_t n = ::readv(fd, vec, iovcnt);

  if (n < 0) {
    *saved_errno = errno;
  } else if (static_cast<size_t>(n) <= writeable) {
    write_index += n;
  } else {
    write_index = buffer.size();
    append(extrabuf, n - writeable);
  }
  return n;
}
```

在栈上准备一个65536字节的extrabuf，然后利用readv来读取数据，iovec有两块，第一块指向Buffer中的writable字节，另一块指向栈上的extrabuf。

这样如果读入的数据不多，那么全部读到Buffer中去，如果超过了writable子结束，就会读到栈上的extrabuf里，然后再把extrabuf的数据append到buffer中。

这样利用了临时栈上空间，避免每个连接的初始Buffer过大造成的内存浪费，也避免反复调用read()的系统开销（由于缓冲区足够大，通畅一次readv系统调用就能读完全部数据）。

#### 14.4 线程安全

Buffer不是线程安全的，但其只会存在于一个工作线程中，所以其不必是线程安全的。

### 15. Socket

```c++
#pragma once
#include <arpa/inet.h>
#include <string>

void setReusePort(int fd);

class ClientSocket {
public:
  ClientSocket() : fd_(-1), addr_{0}, addr_len_(0) {}
  ~ClientSocket();

  socklen_t addr_len_;
  sockaddr_in addr_;
  int fd_;
};

class ServerSocket {
public:
  ServerSocket(int port = 8080, const std::string& p = "");
  ~ServerSocket();
  void bind();
  void listen();
  int accept(ClientSocket&) const;
	
  // 专用socket地址结构
  // struct sockaddr_in
  // {
  //   sa_faimly_t sin_family;  /* 地址族：AF_INET */
  //   u_int16_t sin_port;      /* 端口号，要用网络字节序表示 */
  //   struct in_addr sin_addr; /* IPv4地址结构体 */
  // };
  // struct in_addr
  // {
  //   u_int32_t s_addr;					/* IPv4地址，要用网络字节序表示 */
  // }
  
  sockaddr_in addr_;
  int listen_fd_;
  int epoll_fd_;
  int port_;
  std::string ip_;
};
```

```c++
  addr_ = {0};
  addr_.sin_family = AF_INET;
  addr_.sin_port = htons(port);
  if (!ip.empty()) {
    addr_.sin_addr.s_addr = inet_addr(ip.c_str());
  } else {
    addr_.sin_addr.s_addr = htonl(INADDR_ANY);
  }
  listen_fd_ = socket(AF_INET, SOCK_STREAM, 0);
  setReusePort(listen_fd_);
  setNonblocking(listen_fd_);
  int ret = ::bind(listen_fd_, (struct sockaddr*)&addr_, sizeof(addr_));
  int ret = ::listen(listen_fd_, LINSTENQ);
  int clientfd = ::accept(listen_fd_, (struct sockaddr*)&client_socket.addr_, &client_socket.addr_len_);
```

### 16. setNonBlocking

设置socket为非阻塞

```c
int setNonblocking(int fd) {
    int old_option = ::fcntl(fd, F_GETFL);
    int new_option = old_option | O_NONBLOCK;
    if(::fcntl(fd, F_SETFL, new_option) == -1) {
      LOG_ERROR << "set nonblock failed";
    }
    return old_option;
}
```

### 17. setNoDelay

禁用Nagle算法

```c
void setNoDelay(int fd) {
  int enable = 1;
  setsockopt(fd, IPPROTO_TCP, TCP_NODELAY, (void *)&enable, sizeof(enable));
}
```

### 18. handle_for_sigpipe

连接建立，若某一端关闭连接，而另一端仍然向它写数据，第一次写数据后会收到RST响应，此后再写数据，内核将向进程发出SIGPIPE信号，通知进程此连接已经断开。而SIGPIPE信号的默认处理是终止程序，导致上述问题的发生。应该忽略SIGPIPE信号。

```c
void handle_for_sigpipe() {
  struct sigaction sa;
  memset(&sa, 0, sizeof(sa));
  sa.sa_handler = SIG_IGN;
  sa.sa_flags = 0;
  if (sigaction(SIGPIPE, &sa, NULL)) {
    return;
  }
}
// SIG_IGN 表示忽略目标信号
// SIG_DFL 表示使用信号的默认处理方式
// act 指定新的信号处理方式
// oact 输出线好先前的处理方式
int sigaction(int sig, const struct sigaction* act, struct sigaction* oact);
```

### 19. setReusePort

服务器通过设置socket选项SO_REUSEADDR来强制使用被处于TIME_WAIT状态的连接占用的socket地址。

local peer主动调用close后，此时的TCP连接进入TIME_WAIT状态，处于该状态下的TCP连接不能立即以同样的四元组建立新连接，即发起active close的那方占用的local port在TIME_WAIT期间不能再被重新分配。

```c
void setReusePort(int fd) {
  int optval = 1;
  int ret = setsockopt(fd, SOL_SOCKET, SO_REUSEADDR, (const void*)&optval, sizeof(optval));
  if (ret < 0) {
    LOG_ERROR << "set reuse prot failed in file";
    exit(0);
  }
}
```

之外的socket选项SO_RCVBUF和SO_SNDBUF, SO_RCVLOWAT和SO_SNDLOWAT,SO_LINGER