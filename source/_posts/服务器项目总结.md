---
title: 服务器项目总结
date: 2021-03-05 22:15:36
tags:
    - c++
    - HTTP
    - 项目
categories:
    - 项目相关
---
# Tiny Web Server 服务器项目整理总结

### 1. Reactor模型

主线程（I/O处理单元）只负责监听文件描述上是否有事件发生，有的话就立即将该事件通知工作线程（逻辑单元）。除此之外，主线程不做任何其他实质性的工作。读写数据，接受新的连接，以及处理新的连接，以及处理客户请求均在工作线程中完成。

1. 主线程往epoll内核事件表中注册socket上的读就绪事件。
2. 主线程调用epoll_wait等待socket上有数据可读。
3. 当socket上有数据可读时，epoll_wait通知主线程。主线程则将socket可读事件放入请求队列。
4. 睡眠在请求队列上的某个工作线程被唤醒，他从socket读取数据并处理客户请求，然后往epoll内核事件表中注册该socket上的写就绪事件。
5. 主线程调用epoll_wait()等待socket可写。
6. 当socket可写时，epoll_wait通知主线程。主线程将socket可写事件放入请求队列。
7. 睡眠在请求队列上的某个工作线程被唤醒，它往socket上写入服务器处理客户请求的结果。

### 2. 线程池

+ 资源重用，避免了线程频繁建立、关闭的开销
+ 控制资源的使用。如果不使用池，每次都需要创建一个线程，这样系统的稳定性受系统连接需求影响很大，很容易产生资源浪费和高负载异常。池能够使性能最大化，将资源利用控制在一定的水平之下。连接池能控制池中的连接数量，增强了系统在大量用户应用时的稳定性。
+ 实现：构造、析构、任务enqueue

```c++
#pragma once
#include <vector>
#include <queue>

#include <mutex>
#include <condition_variable>
#include <thread>
#include <functional>
#include <future>
#include <memory>
#include <stdexcept>

class ThreadPool {
public:
  ThreadPool(size_t);
  ~ThreadPool();

  template<typename F, typename... Args>
  auto enqueue(F&&f, Args&&... args)
    -> std::future<typename std::result_of<F(Args...)>::type>;

private:
  std::vector<std::thread> workers;
  std::queue<std::function<void()>> tasks;

  std::mutex queue_mutex;
  std::condition_variable condition;
  bool stop;
};

inline ThreadPool::ThreadPool(size_t num_threads) : stop(false) {
  for (size_t i = 0; i < num_threads; ++i) {
    workers.emplace_back(
      [this](){
        while(true) {
          std::function<void()> task;
          {
            std::unique_lock<std::mutex> lock(this->queue_mutex);
            this->condition.wait(lock,
              [this](){return this->stop || !this->tasks.empty();});
            if (this->stop && this->tasks.empty()) {
              return;
            }
            task = std::move(this->tasks.front());
            this->tasks.pop();
          }
          task();
        }
      }
    );
  }
}

inline ThreadPool::~ThreadPool() {
  {
    std::unique_lock<std::mutex> lock(queue_mutex);
    stop = true;
  }
  condition.notify_all();
  for (std::thread& worker : workers) {
    worker.join();
  }
}

template<typename F, typename... Args>
auto ThreadPool::enqueue(F&& f, Args&&... args)
  -> std::future<typename std::result_of<F(Args...)>::type> {

  using return_type = typename std::result_of<F(Args...)>::type;

  auto task = std::make_shared<std::packaged_task<return_type()>> (
    std::bind(std::forward<F>(f), std::forward<Args>(args)...)
  );

  std::future<return_type> res = task->get_future();
  {
    std::unique_lock<std::mutex> lock(queue_mutex);
    if (stop) {
      throw std::runtime_error("enqueue on stopped ThreadPool");
    }
    tasks.emplace([task](){(*task)();});
  }
  condition.notify_one();
  return res;
}
```

### 3. 状态机解析HTTP请求

LINE_STATE， PARSE_STATE, HTTP_CODE

```c++
	enum LINE_STATE {LINE_OK = 0, LINE_BAD, LINE_MORE};
  enum PARSE_STATE {PARSE_REQUESTLINE = 0, PARSE_HEADER, PARSE_BODY};
  enum HTTP_CODE {NO_REQUEST = 0, GET_REQUEST, BAD_REQUEST, NOT_IMPLEMNTED, 
                  FORBIDDEN_REQUEST, INTERNAL_ERROR, CLOSED_CONNECTION};

  static LINE_STATE parse_line(Buffer& buffer);
  static HTTP_CODE parse_request_line(const std::string&, PARSE_STATE&, HttpRequest&);
  static HTTP_CODE parse_headers(const std::string&, PARSE_STATE&, HttpRequest&);
  static HTTP_CODE parse_body(const std::string&, HttpRequest&);
  static HTTP_CODE parse_content(Buffer& buffer, PARSE_STATE &parse_state, HttpRequest& request);
```

### 4. 定时器结构

最小堆实现。

每一个TimerNode包含一个expired Time，和Httpdata的指针，

有一个_deleted字段支持惰性删除。



#### 实现定时器的其他方式

升序链表、时间轮

### 5. 双缓冲区技术实现异步日志系统

#### 为什么要一个日志系统？

 记录系统状态，方便查找出错原因

#### 为什么要是实现一个日志系统？

stdio在使用上不便，容易出现缓冲区溢出、格式字符串混淆等错误，而且很难保证类型安全。不可扩展。

iostream是可扩展的类型安全的IO机制。但是ostream格式化输出繁琐，istream不适合输入带格式的数据，线程不安全，不适合在多线程程序中做logging。

应该使用成熟的多线程库。

#### 双缓冲区？

准备两块buffer，一块写入日志消息，一块将消息写入文件。 

#### 实现

用一个线程负责收集日志消息并写入文件，其他业务线程只管往这个“日志线程”发送消息，这称为异步日志。

多生产者-单消费者问题，对于生产者而言，要尽量做到低延迟，低cpu开销，无阻塞；对消费者而言要做到足够大的吞吐量，并占用较少资源。

+ FileUtil 封装了文件的打开，关闭，写入。
+  LogFile 在FileUtil的基础上，加了锁，和按写次数flush。
+ LogStream 重载operator<<,将内容存在自己的缓存中。
+  AsyncLogging 启动一个log线程，负责定时或缓冲区填满时，将log写入logFile。双缓冲交换指针。
+  Logging 是对外接口，内含一个LogStream，将内容输入到LogStream的buffer中，并有一个静态的AsyncLogger， 向日志文件内写内容。实现了日志等级

### 6. RAII机制

资源获得即初始化。

智能指针、unique_lock、fd（socket）

### 7. 压测

Webbench。4内核、4G内存。1000clients，60s，QPS 36621
内部实现有两种机制，一种是等待结果计算再开始下一次请求。另一种不等待计算结果直接开始下一次请求

会问到HTTP管线化（HTTP pipelining）

进程间通信(pipe)：

管道：如果没有缓冲区，单纯的往其中放入元素立马就会进入阻塞状态，必须有其他的线程从其中取走元素。通俗的讲要有一个线程不断的取这个管道的元素，才能往其中放入元素。它就像一个窄窄的门框，进去就得出来。

而有一个缓冲区的管道想一段地道，放入的元素不会马上进入阻塞状态，只有第二个准备进入而第一个还没有进入的情况下才会阻塞。

### 8. 一个请求到来的处理过程

启动：服务器listen_fd, 新建线程池，和epoll_fd。 令epoll监听listen_fd(不要注册EPOLLONESHOT）.

请求到来：connect，epoll通知事件就绪，将client_socket设置非阻塞，Nodelay（禁用nagle算法），将fd和httpData绑定，然后重新加入epoll，添加定时器。将处理连接请求的函数推入线程池的任务队列。

处理请求：

+ 从socket中读取数据，逐步分析请求行（方法，url，版本）首部行，空行，实体主体。
+ 如果是HEAD请求，发送头部，如果是GET，除了头部之外在发送指定文件。（共享内存，mmap，munmap）
+ 如果是短链接则直接关闭，长连接重新添加定时器，再次令epoll监听fd

### 9 遇到的问题

1. webbench测试完就会宕机。

连接建立，若某一端关闭连接，而另一端仍然向它写数据，第一次写数据后会收到RST响应，此后再写数据，内核将向进程发出SIGPIPE信号，通知进程此连接已经断开。而SIGPIPE信号的默认处理是终止程序，导致上述问题的发生。应该忽略SIGPIPE信号。

### 10. 为什么用非阻塞IO

- 水平触发：只要缓冲区有数据准备好就传递就绪信号
- 边缘触发：只有新数据到来才会传递就绪信号，没有新数据到来时尽管有旧数据没有呗读取也不通知

ET模式在很大程度上减少了epoll事件被重复触发的次数，因此效率要比LT模式高。epoll工作在ET模式的时候，必须使用非阻塞套接口，以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。

### 11. 无锁队列

CAS

### 12. epoll

epoll是select和poll的增强版本。相对于select和poll来说，epoll更加的灵活，没有描述符的限制。epoll使用一个文件描述符管理多个描述符，将用户关系的文件描述符放到内核一个事件表中，这样用户空间和内核空间的数据拷贝只需要一次。

```c++
#include <sys/epoll.h>
int epoll_create(int size);
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
int epoll_wait(int epfd, struct epoll_event* events, int maxevents, int timeout);
```

epoll_create 创建一个epoll句柄，size用来告诉内核监听的数目。这个不同于select中的第一个参数。需要注意的是，创建好epoll句柄后，他会占用一个fd值，所以使用完后要关闭

epoll_ctl 事件注册函数，不同于select() 监听事件时告诉内核监听什么样的事件， 而是先注册监听事件的类型。第一个参数是epoll_create()的返回值，第二个参数表示动作。（CTL的增删改）第三个参数是需要监听的参数，第四个参数告诉内核需要监听什么事。events是几个宏 的集合（类似poll）

- 水平触发：只要缓冲区有数据准备好就传递就绪信号
- 边缘触发：只有新数据到来才会传递就绪信号，没有新数据到来时尽管有旧数据没有被读取也不通知

ET模式在很大程度上减少了epoll事件被重复触发的次数，因此效率要比LT模式高。epoll工作在ET模式的时候，必须使用非阻塞套接口，以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。

#### EPOLLONESHOT事件

即使使用ET模式，一个socket上的某个事件还是可能被多次触发。比如，一个线程在读取完某个socket上的数据开始处理这些数据，而在数据的处理过程中该socket上又有新数据可读（EPOLLIN再次被触发）此时另外一个线程被唤醒来读取这些新的数据。于是出现了两个线程同时擦耦走一个socket的局面。我们希望**一个socket连接在任意时刻都只被一个线程处理**。这时要使用epoll的EPOLLONESHOT事件实现。

对于注册了POLLONESHOT事件的文件描述符，系统最多触发一个事件，且只触发一次，**除非用epoll_ctl函数重置**。当一个线程在处理某个socket时，其他线程是不可能有机会操作该socket的。操作完之后应重置EPOLLONESHOT事件。如果再处理时有新来了请求该线程将继续为这个socket服务。

监听socket listen_fd 上是不能注册EPOLLONESHOT事件的，否则应用程序只能处理一个客户连接。

后续的客户连接请求将不再触发listenfd上的EPOLLIN事件



#### select poll epoll区别

- select

1. select函数使用整形变量保存当前管理的用户进程数，使用句柄指针向可读进程集合、可写进程集合以及错误进程集合，使用毫秒级定时器结构体进程轮询计时。

- poll

1. 使用事件代码代替select可读可写错误指针，能够监听更多种事件，将输入输出事件分开，下次进入时不必重新初始化
2. 计时器采用整形变量，计时精度变低，只能执行秒级计时。
3. 可监控的数据量远大于select。

- epoll

1. 定时器超时后不必全局扫描管理的进程，epoll函数只对活跃的socket进行操作，每个socket的数据准备好后会调用回调函数使socket变为活跃。
2. 使用mmap共享内存加速内核和用户空间消息传递

### 13. 多进程和多线程的适用场景

1. 需要频繁创建和销毁的优先使用线程,
2. 强相关的处理使用线程，弱相关使用进程。当数据是共享且可变的，那么用多进程之间的数据同步会成为很大问题。
3. 可能扩展到多机分布的使用进程，多核分布的使用线程

#### 1. 多进程的工作模式
  + a. 将一个单线程的进程运行多份
  + b. 主进程 + worker进程，比如httpd + fastcgi

+ 在进行启动和销毁的开销远小于实际任务的耗时时，使用进程是合理的。如果任务耗时较少，可以起一个线程。

#### 2. 必须使用单线程的场合
  1. 程序可能会fork();
  2. 限制程序的CPU占用率。
  3. 相比单线程，在IObound（很小的CPU负载便能将IO跑慢）和CPUbound（很小的IO流量就能让CPU跑慢）的场合下，多线程用处不大

#### 3. 使用多线程程序的场景

  提高响应速度，让IO和“计算”相互重叠，降低latency。虽然多线程不能提高绝对性能，但能提高平均响应性能。